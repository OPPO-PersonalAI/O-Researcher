# =============================================================================
#                         O-Researcher Configuration File
# =============================================================================
# Usage: 
#   1. Copy this file to .env:  cp env_template .env
#   2. Edit .env and fill in actual values
# =============================================================================


# =============================================================================
# Server Configuration (server/start_servers.sh)
# =============================================================================

export SERVER_HOST="xx.xx.xx.xx"          # Server listening address
export CRAWL_PAGE_PORT="20001"          # CrawlPage service port
export WEBSEARCH_PORT="20002"           # WebSearch service port
export CRAWL_PAGE_WORKERS=10            # CrawlPage worker processes
export WEBSEARCH_WORKERS=10             # WebSearch worker processes


# =============================================================================
# API Configuration
# =============================================================================

export SUMMARY_API_URLS="https://your-api-url.com/v1"   # API URL (multiple separated by |)
export SUMMARY_OPENAI_API_KEY="sk-your-api-key"         # OpenAI API Key
export SUMMARY_MODEL="gpt-5-mini"                       # Model name to use
export SERP_API_KEY="your-serp-api-key"                 # Serper API Key
export JINA_API_KEY="your-jina-api-key"                 # Jina API Key


# =============================================================================
# Model Deployment Configuration (deploy/deploy.sh)
# =============================================================================

export MODEL_PATH="/path/to/your/model"     # Model path (required)
export MODEL_NAME="your_model_name"         # Model name (required)
export MODEL_BASE_PORT="9095"               # Model service base port
export DEPLOY_HOST="0.0.0.0"                # Deployment listening address

# Deployment parameters (optional, have defaults)
export DEPLOY_INSTANCES=1                   # Number of deployment instances
export DEPLOY_GPUS_PER_INSTANCE=4           # GPUs per instance
export DEPLOY_MAX_MODEL_LEN=131072          # Maximum model length
export DEPLOY_WAIT_TIMEOUT=300              # Startup timeout (seconds)
export DEPLOY_LOG_DIR="deploy/logs"         # Deployment log directory

# =============================================================================
# Inference Configuration (infer/infer.py)
# =============================================================================

# Model API URL (multiple instances separated by |, auto round-robin load balancing)
# Note: If DEPLOY_INSTANCES > 1, configure multiple ports
#       Ports increment from MODEL_BASE_PORT
#       Example: DEPLOY_INSTANCES=2, MODEL_BASE_PORT=9095 â†’ ports 9095, 9096
#
# Single instance example (DEPLOY_INSTANCES=1):
export MODEL_URL="http://xx.xx.xx.xx:9095/v1"
# Dual instance example (DEPLOY_INSTANCES=2):
# export MODEL_URL="http://xx.xx.xx.xx:9095/v1|http://xx.xx.xx.xx:9096/v1"
# Triple instance example (DEPLOY_INSTANCES=3):
# export MODEL_URL="http://xx.xx.xx.xx:9095/v1|http://xx.xx.xx.xx:9096/v1|http://xx.xx.xx.xx:9097/v1"

# Tool service URLs
export WEBSEARCH_URL="http://xx.xx.xx.xx:20001/search"
export CRAWL_PAGE_URL="http://xx.xx.xx.xx:20002/crawl_page"

# =============================================================================
# System Optimization (optional)
# =============================================================================

export OMP_NUM_THREADS=16                  # OpenMP thread count
export TORCHDYNAMO_VERBOSE=1               # TorchDynamo debug output
export VLLM_USE_V1=1                       # Use vLLM v1 version
